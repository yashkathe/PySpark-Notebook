{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMuoTtmdpn3f03aK3CsHC7R"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CHXB1JgHw04v",
        "outputId": "fb2c3be3-effd-4a9b-a37c-269b7ad87732"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests"
      ],
      "metadata": {
        "id": "zdBjMw23w6i5"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Dowload CSV File\n",
        "\n",
        "def download_file(url, filename):\n",
        "\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status()\n",
        "        with open(filename, 'wb') as f:\n",
        "            f.write(response.content)\n",
        "        print(f\"File downloaded successfully and saved as {filename}\")\n",
        "    except requests.exceptions.HTTPError as errh:\n",
        "        print(f\"HTTP Error: {errh}\")\n",
        "    except requests.exceptions.ConnectionError as errc:\n",
        "        print(f\"Error Connecting: {errc}\")\n",
        "    except requests.exceptions.Timeout as errt:\n",
        "        print(f\"Timeout Error: {errt}\")\n",
        "    except requests.exceptions.RequestException as err:\n",
        "        print(f\"OOps: Something Else: {err}\")"
      ],
      "metadata": {
        "id": "BnU4POs1w-2l"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url = 'https://drive.google.com/uc?id=1phaHg9objxK2MwaZmSUZAKQ8kVqlgng4&export=download'\n",
        "filename = 'data.csv'\n",
        "\n",
        "download_file(url, filename)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yudvo0cwxD61",
        "outputId": "13ebc30f-a6bc-497e-ce7e-2214b66512a2"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File downloaded successfully and saved as data.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession"
      ],
      "metadata": {
        "id": "ZCrZ35hWxF14"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark = SparkSession.builder.appName('DataFrame').getOrCreate()\n",
        "spark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "qwpKqTGMxHSW",
        "outputId": "8a63360c-4c42-4f63-bff8-4f7a9da7bc0e"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x79fa1ff16ce0>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://e2a3d018a85d:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.5.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>DataFrame</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# consider first row as header\n",
        "df_pyspark = spark.read.option('header', 'true').csv('data.csv')\n",
        "df_pyspark.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fiLdH7KlxKDo",
        "outputId": "2c9dbc48-16a5-4426-ff12-7e2177fd3ca8"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+---------------+----------+---------+------+--------------------+--------------------+-------------+------------------+\n",
            "|Index|        User Id|First Name|Last Name|   Sex|               Email|               Phone|Date of birth|         Job Title|\n",
            "+-----+---------------+----------+---------+------+--------------------+--------------------+-------------+------------------+\n",
            "|    1|88F7B33d2bcf9f5|    Shelby|  Terrell|  Male|elijah57@example.net|001-084-906-7849x...|   1945-10-26|   Games developer|\n",
            "|    2|f90cD3E76f1A9b9|   Phillip|  Summers|Female|bethany14@example...|   214.112.6044x4913|   1910-03-24|    Phytotherapist|\n",
            "|    3|DbeAb8CcdfeFC2c|  Kristine|   Travis|  Male|bthompson@example...|        277.609.7938|   1992-07-02|         Homeopath|\n",
            "|    4|A31Bee3c201ef58|   Yesenia| Martinez|  Male|kaitlinkaiser@exa...|        584.094.6111|   2017-08-03| Market researcher|\n",
            "|    5|1bA7A3dc874da3c|      Lori|     Todd|  Male|buchananmanuel@ex...|   689-207-3558x7233|   1938-12-01|Veterinary surgeon|\n",
            "+-----+---------------+----------+---------+------+--------------------+--------------------+-------------+------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_pyspark.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FUZY0GDXykXZ",
        "outputId": "bf72e9b7-daa6-447d-f529-d93e0a793319"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- Index: string (nullable = true)\n",
            " |-- User Id: string (nullable = true)\n",
            " |-- First Name: string (nullable = true)\n",
            " |-- Last Name: string (nullable = true)\n",
            " |-- Sex: string (nullable = true)\n",
            " |-- Email: string (nullable = true)\n",
            " |-- Phone: string (nullable = true)\n",
            " |-- Date of birth: string (nullable = true)\n",
            " |-- Job Title: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dropping Columns"
      ],
      "metadata": {
        "id": "EPvjzRxZyprz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_pyspark.drop('Sex', 'Phone').show(2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fcUjVWEnytzv",
        "outputId": "10bcb525-1a09-4ad4-e9e7-b3a40ef3dbd0"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+---------------+----------+---------+--------------------+-------------+---------------+\n",
            "|Index|        User Id|First Name|Last Name|               Email|Date of birth|      Job Title|\n",
            "+-----+---------------+----------+---------+--------------------+-------------+---------------+\n",
            "|    1|88F7B33d2bcf9f5|    Shelby|  Terrell|elijah57@example.net|   1945-10-26|Games developer|\n",
            "|    2|f90cD3E76f1A9b9|   Phillip|  Summers|bethany14@example...|   1910-03-24| Phytotherapist|\n",
            "+-----+---------------+----------+---------+--------------------+-------------+---------------+\n",
            "only showing top 2 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Deleting entire row with null value in any cell\n",
        "\n",
        "`.na.drop()` method is specifically designed to remove rows with null values"
      ],
      "metadata": {
        "id": "TRhoF5bXy9jl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_pyspark.na.drop() # thats it"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n3d5fOOQy9Hf",
        "outputId": "ea2f94d2-83f8-4e78-d70b-12d975d84348"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[Index: string, User Id: string, First Name: string, Last Name: string, Sex: string, Email: string, Phone: string, Date of birth: string, Job Title: string]"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Understanding .na.drop() Arguments\n",
        "\n",
        "Additionally:  \n",
        "`.na.drop()` takes 3 argument ->\n",
        "1. how = any / all (default any)\n",
        "2. thresh\n",
        "3. subset"
      ],
      "metadata": {
        "id": "yOj1IglHzIUS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_pyspark.na.drop(how=\"all\") # if ALL VALUES in a row are null -> drop the row"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bW-PD9Yay7PK",
        "outputId": "e3f8b5af-5db5-45d4-8500-cfbc66dbff5c"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[Index: string, User Id: string, First Name: string, Last Name: string, Sex: string, Email: string, Phone: string, Date of birth: string, Job Title: string]"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_pyspark.na.drop(how=\"any\", thresh=2) # thresh says ALLEAST 2 NON-NULL values should be present -> else drop"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VZ0fsKEy0ExF",
        "outputId": "22446a96-73fe-427d-d407-6593cb0c59f2"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[Index: string, User Id: string, First Name: string, Last Name: string, Sex: string, Email: string, Phone: string, Date of birth: string, Job Title: string]"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_pyspark.na.drop(how=\"any\", subset=['First Name']) # if there are any null records in row mentioned in subset -> drop"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n6yzj0FD0iBa",
        "outputId": "16cfb0f3-821d-464d-db7b-95e5e0e2fd33"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[Index: string, User Id: string, First Name: string, Last Name: string, Sex: string, Email: string, Phone: string, Date of birth: string, Job Title: string]"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Filling the missing values"
      ],
      "metadata": {
        "id": "JAqLH3AU1ZtM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Full NULL values with 'Not Available'\n",
        "\n",
        "df_pyspark.na.fill('Not Available')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T9x4EPsS1bLl",
        "outputId": "02bc301e-bf04-4760-fc28-13342c4129af"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[Index: string, User Id: string, First Name: string, Last Name: string, Sex: string, Email: string, Phone: string, Date of birth: string, Job Title: string]"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# For specific columns\n",
        "\n",
        "df_pyspark.na.fill('Not Available', ['Sex', 'Email'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rz3SCO_H1sNK",
        "outputId": "eb3ff223-faaa-4ad6-af22-e406984b731c"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[Index: string, User Id: string, First Name: string, Last Name: string, Sex: string, Email: string, Phone: string, Date of birth: string, Job Title: string]"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imputer Functions\n",
        "\n",
        "Suppose we can fill null values with mean, median, mode of that column"
      ],
      "metadata": {
        "id": "mM5qzpTz2Q8C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import Imputer"
      ],
      "metadata": {
        "id": "QV4XVGaU3PEb"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example Usage\n",
        "\n",
        "\"\"\"\n",
        "imputer = Imputer(\n",
        "    inputCols=['age', 'Experience', 'Salary'],\n",
        "    outputCols=[\"{}_imputed\".format(c) for c in ['age', 'Experience', 'Salary']]\n",
        "    ).setStrategy(\"median\")\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "8hFzscY910Ef",
        "outputId": "ce724254-c5d6-4a17-fd51-0c8982203204"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nimputer = Imputer(\\n    inputCols=[\\'age\\', \\'Experience\\', \\'Salary\\'], \\n    outputCols=[\"{}_imputed\".format(c) for c in [\\'age\\', \\'Experience\\', \\'Salary\\']]\\n    ).setStrategy(\"median\")\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    }
  ]
}