{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UgaeU6yN8SED",
        "outputId": "0c074a46-1c2b-44ab-df5f-5cd7a0426ea9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488491 sha256=bfb4ace981627e35b5f6e3d8b5cf4f2bd669d8c9ecdeff2df41d9cdf5d8ba33b\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/1d/60/2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.1\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests"
      ],
      "metadata": {
        "id": "r1ep0tw580Rl"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def download_file(url, filename):\n",
        "\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status()\n",
        "        with open(filename, 'wb') as f:\n",
        "            f.write(response.content)\n",
        "        print(f\"File downloaded successfully and saved as {filename}\")\n",
        "    except requests.exceptions.HTTPError as errh:\n",
        "        print(f\"HTTP Error: {errh}\")\n",
        "    except requests.exceptions.ConnectionError as errc:\n",
        "        print(f\"Error Connecting: {errc}\")\n",
        "    except requests.exceptions.Timeout as errt:\n",
        "        print(f\"Timeout Error: {errt}\")\n",
        "    except requests.exceptions.RequestException as err:\n",
        "        print(f\"OOps: Something Else: {err}\")"
      ],
      "metadata": {
        "id": "K9rcX_C180PP"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url = 'https://gist.githubusercontent.com/kevin336/acbb2271e66c10a5b73aacf82ca82784/raw/e38afe62e088394d61ed30884dd50a6826eee0a8/employees.csv'\n",
        "filename = 'data.csv'\n",
        "\n",
        "download_file(url, filename)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bwUE07uW80Mj",
        "outputId": "a1bd8773-963a-44e6-f519-212b44188612"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File downloaded successfully and saved as data.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession"
      ],
      "metadata": {
        "id": "PDfNqw5L80J_"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark = SparkSession.builder.appName('DataFrame').getOrCreate()\n",
        "spark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "ecUaz0x18z6D",
        "outputId": "0d9296fc-b549-4501-8245-5b3e4ba8aefb"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7b6ca4e1b670>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://eb1cc3dc09a4:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.5.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>DataFrame</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_pyspark = spark.read.csv('data.csv', header=True, inferSchema=True)\n",
        "df_pyspark.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_YELtTSX89nT",
        "outputId": "669d7a96-f611-4a5c-fad1-7b238488d9c3"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+----------+---------+--------+------------+---------+--------+------+--------------+----------+-------------+\n",
            "|EMPLOYEE_ID|FIRST_NAME|LAST_NAME|   EMAIL|PHONE_NUMBER|HIRE_DATE|  JOB_ID|SALARY|COMMISSION_PCT|MANAGER_ID|DEPARTMENT_ID|\n",
            "+-----------+----------+---------+--------+------------+---------+--------+------+--------------+----------+-------------+\n",
            "|        198|    Donald| OConnell|DOCONNEL|650.507.9833|21-JUN-07|SH_CLERK|  2600|            - |       124|           50|\n",
            "|        199|   Douglas|    Grant|  DGRANT|650.507.9844|13-JAN-08|SH_CLERK|  2600|            - |       124|           50|\n",
            "|        200|  Jennifer|   Whalen| JWHALEN|515.123.4444|17-SEP-03| AD_ASST|  4400|            - |       101|           10|\n",
            "|        201|   Michael|Hartstein|MHARTSTE|515.123.5555|17-FEB-04|  MK_MAN| 13000|            - |       100|           20|\n",
            "|        202|       Pat|      Fay|    PFAY|603.123.6666|17-AUG-05|  MK_REP|  6000|            - |       201|           20|\n",
            "+-----------+----------+---------+--------+------------+---------+--------+------+--------------+----------+-------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GROUP BY and AGGREGRATE\n",
        "work together"
      ],
      "metadata": {
        "id": "3pHPGplO9RrF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# max salary for for each job\n",
        "df_pyspark.groupBy('JOB_ID').max('SALARY').show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "31aB1yY39QCZ",
        "outputId": "bf083b50-fc94-44a1-9a3f-46b195ad5e3a"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----------+\n",
            "|    JOB_ID|max(SALARY)|\n",
            "+----------+-----------+\n",
            "|FI_ACCOUNT|       9000|\n",
            "|    MK_MAN|      13000|\n",
            "|   IT_PROG|       9000|\n",
            "|    FI_MGR|      12008|\n",
            "|AC_ACCOUNT|       8300|\n",
            "|    HR_REP|       6500|\n",
            "|  PU_CLERK|       3100|\n",
            "|    AC_MGR|      12008|\n",
            "|    PR_REP|      10000|\n",
            "|    ST_MAN|       8200|\n",
            "|    MK_REP|       6000|\n",
            "|    PU_MAN|      11000|\n",
            "|  SH_CLERK|       2600|\n",
            "|   AD_PRES|      24000|\n",
            "|   AD_ASST|       4400|\n",
            "|  ST_CLERK|       3600|\n",
            "|     AD_VP|      17000|\n",
            "+----------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# how much salary each department gives\n",
        "df_pyspark.groupBy('DEPARTMENT_ID').sum('SALARY').show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bzAu_uh49pPQ",
        "outputId": "5468b86d-bf0c-4f7e-a510-c9d4853702fd"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+-----------+\n",
            "|DEPARTMENT_ID|sum(SALARY)|\n",
            "+-------------+-----------+\n",
            "|           20|      19000|\n",
            "|           40|       6500|\n",
            "|          100|      51608|\n",
            "|           10|       4400|\n",
            "|           50|      85600|\n",
            "|           70|      10000|\n",
            "|           90|      58000|\n",
            "|           60|      28800|\n",
            "|          110|      20308|\n",
            "|           30|      24900|\n",
            "+-------------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# count people for each position\n",
        "df_pyspark.groupBy('JOB_ID').count().show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UqXpfSGk9pMs",
        "outputId": "53a49e7b-8768-460d-88f8-13866e5c7d15"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----+\n",
            "|    JOB_ID|count|\n",
            "+----------+-----+\n",
            "|FI_ACCOUNT|    5|\n",
            "|    MK_MAN|    1|\n",
            "|   IT_PROG|    5|\n",
            "|    FI_MGR|    1|\n",
            "|AC_ACCOUNT|    1|\n",
            "|    HR_REP|    1|\n",
            "|  PU_CLERK|    5|\n",
            "|    AC_MGR|    1|\n",
            "|    PR_REP|    1|\n",
            "|    ST_MAN|    5|\n",
            "|    MK_REP|    1|\n",
            "|    PU_MAN|    1|\n",
            "|  SH_CLERK|    2|\n",
            "|   AD_PRES|    1|\n",
            "|   AD_ASST|    1|\n",
            "|  ST_CLERK|   16|\n",
            "|     AD_VP|    2|\n",
            "+----------+-----+\n",
            "\n"
          ]
        }
      ]
    }
  ]
}